
layout: default
title: Dawn Brewer — Product & UX Portfolio

# AI Reporting & Audit Flow

I design human-centered systems that make AI risk, data compliance, and decision-making visible **before deployment**, not after.



## The Problem

AI teams often discover licensing, data provenance, or compliance issues too late — when fixes are expensive or impossible.

Legal teams lack visibility.  
Product teams lack clear signals.  
Risk shows up at the worst possible time.


## The Solution

An AI reporting flow that surfaces risk early, explains it clearly, and routes it to the right humans at the right moment.


## How It Works

1. Training data is continuously evaluated for licensing and source coverage  
2. Risk signals are summarized in a clear audit view  
3. Issues can be escalated to legal or policy teams before deployment  
4. Teams ship with confidence — or pause with evidence
 ### Audit View (Concept)

![Annotated audit view showing risk summary, source coverage,
and escalation status](assets/ai-audit-annotated.png)

## Trust Signals

This project was designed and evaluated with quality, risk,
and real-world use in mind — not just visual polish.

- ✔️ Early risk detection during training, not deployment
- ✔️ Clear audit summaries for legal and policy review
- ✔️ Human-in-the-loop escalation (no black-box decisions)
- ✔️ Plain-language explanations for non-technical users
- ✔️ Explicit handling of incomplete or uncertain data
### What I didn’t test (yet)

To be transparent, this MVP does not yet cover:

- Large-scale, multi-team permission models
- Cross-region regulatory differences
- Automated exports for external regulators

These were intentionally scoped out to focus on
clarity, decision timing, and human review workflows first.
---

**Why this matters**

AI systems don’t fail only because of bad models —
they fail because teams don’t see risk in time.

This project focuses on making risk visible,
actionable, and reviewable while teams can still act.
